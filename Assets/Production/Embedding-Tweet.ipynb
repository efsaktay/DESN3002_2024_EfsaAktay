{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e48c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "th = torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37ece67-ddf8-400d-aa16-5c7a375ea906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date Created', 'Tweet'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "FILEPATH = '/Users/Shared/tweets/elon_tweet.csv'\n",
    "df1 = pd.read_csv(FILEPATH)\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eace1e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "embedder = BertModel.from_pretrained('bert-base-uncased')\n",
    "text = 'TWEETS'\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "# output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefe15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4643498a",
   "metadata": {},
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c463bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alert', 'alert', '', 'none', 'none', 'none', 'none', 'alert', '', ''], ['none', 'none', 'report', 'report', 'report', 'report', 'report', 'none', 'report', 'announcement']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, dataframes, input_window, output_window):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        dataframes (list of pd.DataFrame): List of dataframes, each containing columns:\n",
    "                                          'time', 'stock price', 'stock index price', 'text message'\n",
    "        input_window (int): Number of timesteps in each input sequence.\n",
    "        output_window (int): Number of timesteps in each target sequence.\n",
    "        \"\"\"\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.sequences = []\n",
    "        self.text_encoder = LabelEncoder()\n",
    "\n",
    "        # Preprocess and encode text data from all dataframes\n",
    "        # all_texts = pd.concat([df['text message'] for df in dataframes]).values()\n",
    "        all_texts = [_df['text message'].values.tolist() for _df in dataframes]\n",
    "        list_list_embs = self.embed_text(all_texts)\n",
    "\n",
    "        \n",
    "        for _i_df, df in enumerate(dataframes):\n",
    "            stock_prices = th.tensor(df['stock price'].values).reshape(-1, 1)\n",
    "            index_prices = th.tensor(df['stock index price'].values).reshape(-1, 1)\n",
    "            embs = list_list_embs[_i_df]\n",
    "            \n",
    "            num_sequences = len(df) - self.input_window - self.output_window + 1\n",
    "\n",
    "            for i in range(num_sequences):\n",
    "                input_start = i\n",
    "                input_end = i + self.input_window\n",
    "                target_start = input_end\n",
    "                target_end = target_start + self.output_window\n",
    "    \n",
    "                a = stock_prices[input_start:input_end]\n",
    "                b = index_prices[input_start:input_end]\n",
    "                c = embs[input_start:input_end]\n",
    "                \n",
    "                input_sequence = th.cat((a, b, c), dim=1)\n",
    "                target_sequence = stock_prices[target_start:target_end]\n",
    "                self.sequences.append((input_sequence, target_sequence))\n",
    "                \n",
    "    def embed_text(self, list_list_texts):\n",
    "        unique_embs = dict()\n",
    "        print(list_list_texts)\n",
    "        \n",
    "        embs = []\n",
    "        for _i, _txts in enumerate(list_list_texts):\n",
    "            _embs = []\n",
    "            for _txt in _txts:\n",
    "                if _txt in unique_embs: \n",
    "                    _emb = unique_embs.get(_txt)\n",
    "                else:\n",
    "                    _tkn = tokenizer(_txt, return_tensors='pt') \n",
    "                    _emb = embedder(**_tkn).last_hidden_state[:,0,:]\n",
    "                    unique_embs[_txt] = _emb\n",
    "                _embs.append(_emb)\n",
    "            embs.append(_embs)\n",
    "\n",
    "        list_list_embs = [th.cat(_embs, dim=0) for _embs in embs]\n",
    "        return list_list_embs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence, target_sequence = self.sequences[idx]\n",
    "        input_sequence = torch.tensor(input_sequence, dtype=torch.float32)\n",
    "        target_sequence = torch.tensor(target_sequence, dtype=torch.float32)\n",
    "        return input_sequence, target_sequence\n",
    "\n",
    "# Example fake data generation\n",
    "np.random.seed(42)\n",
    "data_length = 10\n",
    "df1 = pd.DataFrame({\n",
    "    'time': pd.date_range(start='1/1/2020', periods=data_length, freq='D'),\n",
    "    'stock price': np.random.rand(data_length) * 100,\n",
    "    'stock index price': np.random.rand(data_length) * 1000,\n",
    "    'text message': np.random.choice(['news', '', 'alert', 'none'], data_length)\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'time': pd.date_range(start='1/1/2020', periods=data_length, freq='D'),\n",
    "    'stock price': np.random.rand(data_length) * 100,\n",
    "    'stock index price': np.random.rand(data_length) * 1000,\n",
    "    'text message': np.random.choice(['announcement', 'report', 'alert', 'none'], data_length)\n",
    "})\n",
    "\n",
    "dataframes = [df1, df2]\n",
    "input_window = 5  # e.g., 5 days input\n",
    "output_window = 2  # e.g., 2 days to predict\n",
    "dataset = StockDataset(dataframes, input_window, output_window)\n",
    "\n",
    "# ### Iterate over the DataLoader\n",
    "# for inputs, targets in dataloader:\n",
    "#     print(\"Inputs Shape:\", inputs.shape)  # Shape should be [batch_size, input_window, features]\n",
    "#     print(\"Targets Shape:\", targets.shape)  # Shape should be [batch_size, output_window]\n",
    "#     break  # Only print the first batch to check\n",
    ","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07baa014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 3.7454e+01,  2.0584e+01, -2.6599e-01,  ..., -1.6488e-01,\n",
      "          2.8372e-02,  3.1597e-01],\n",
      "        [ 9.5071e+01,  9.6991e+02, -2.6599e-01,  ..., -1.6488e-01,\n",
      "          2.8372e-02,  3.1597e-01],\n",
      "        [ 7.3199e+01,  8.3244e+02, -1.0534e+00,  ..., -3.7310e-01,\n",
      "         -3.8231e-01,  3.3689e-01],\n",
      "        [ 5.9866e+01,  2.1234e+02, -1.9510e-01,  ..., -4.3778e-02,\n",
      "          6.9668e-02,  1.9106e-01],\n",
      "        [ 1.5602e+01,  1.8182e+02, -1.9510e-01,  ..., -4.3778e-02,\n",
      "          6.9668e-02,  1.9106e-01]]), tensor([[15.5995],\n",
      "        [ 5.8084]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f0/ckkb_ykj483d49h__2mxzsk00000gn/T/ipykernel_14054/776038003.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_sequence = torch.tensor(input_sequence, dtype=torch.float32)\n",
      "/var/folders/f0/ckkb_ykj483d49h__2mxzsk00000gn/T/ipykernel_14054/776038003.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_sequence = torch.tensor(target_sequence, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ece666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### DataLoader Setup\n",
    "batch_size = 10\n",
    "shuffle = True\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e63d77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['none',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'news',\n",
       " 'none',\n",
       " 'none',\n",
       " '',\n",
       " 'none',\n",
       " 'none',\n",
       " '',\n",
       " 'alert',\n",
       " 'none',\n",
       " '',\n",
       " 'news',\n",
       " 'alert',\n",
       " 'news',\n",
       " 'news',\n",
       " '',\n",
       " 'none',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'alert',\n",
       " 'none',\n",
       " 'news',\n",
       " 'news',\n",
       " 'none',\n",
       " 'news',\n",
       " 'none',\n",
       " 'news',\n",
       " '',\n",
       " 'news',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'alert',\n",
       " 'news',\n",
       " 'none',\n",
       " 'news',\n",
       " 'none',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'alert',\n",
       " '',\n",
       " 'alert',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'news',\n",
       " 'alert',\n",
       " '',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'alert',\n",
       " '',\n",
       " 'news',\n",
       " '',\n",
       " 'news',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'none',\n",
       " 'news',\n",
       " 'news',\n",
       " 'none',\n",
       " 'news',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'none',\n",
       " 'alert',\n",
       " 'none',\n",
       " 'news',\n",
       " 'none',\n",
       " 'news',\n",
       " 'none',\n",
       " 'none',\n",
       " '',\n",
       " 'news',\n",
       " '',\n",
       " 'news',\n",
       " '',\n",
       " 'alert',\n",
       " 'none',\n",
       " 'news',\n",
       " 'news',\n",
       " 'none',\n",
       " 'news',\n",
       " 'news',\n",
       " '',\n",
       " 'news',\n",
       " 'alert',\n",
       " 'alert',\n",
       " 'none',\n",
       " 'news',\n",
       " 'none']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['text message'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c550ae98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      none\n",
       "1      none\n",
       "2     alert\n",
       "3      news\n",
       "4      none\n",
       "      ...  \n",
       "95    alert\n",
       "96    alert\n",
       "97     none\n",
       "98     news\n",
       "99     none\n",
       "Name: text message, Length: 156, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = pd.concat([df1['text message'] for df in dataframes])\n",
    "non_empty_texts = all_texts[all_texts.str.strip().astype(bool)]\n",
    "non_empty_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091d97f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 37.4540,  31.4292,   3.0000],\n",
       "         [ 95.0714, 636.4104,   3.0000],\n",
       "         [ 73.1994, 314.3560,   0.0000],\n",
       "         [ 59.8658, 508.5707,   2.0000],\n",
       "         [ 15.6019, 907.5665,   3.0000]]),\n",
       " tensor([15.5995,  5.8084]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f83a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
